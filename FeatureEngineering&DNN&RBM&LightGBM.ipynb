{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\lenovo\\miniforge3\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\lenovo\\miniforge3\\lib\\site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\lenovo\\miniforge3\\lib\\site-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\lenovo\\miniforge3\\lib\\site-packages (from seaborn) (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\lenovo\\miniforge3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\lenovo\\miniforge3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\lenovo\\miniforge3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\lenovo\\miniforge3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\lenovo\\miniforge3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\lenovo\\miniforge3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lenovo\\miniforge3\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lenovo\\miniforge3\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\lenovo\\miniforge3\\lib\\site-packages (8.1.5)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (8.29.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in c:\\users\\lenovo\\miniforge3\\lib\\site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in c:\\users\\lenovo\\miniforge3\\lib\\site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: decorator in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: lightgbm in c:\\users\\lenovo\\miniforge3\\lib\\site-packages (4.5.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\lenovo\\miniforge3\\lib\\site-packages (from lightgbm) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\lenovo\\miniforge3\\lib\\site-packages (from lightgbm) (1.11.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn\n",
    "!pip install ipywidgets\n",
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import random\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "#import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "from datetime import datetime\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix\n",
    "from ipywidgets import interact\n",
    "from IPython.display import Image\n",
    "\n",
    "# Scikit-Learn Imports\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import lightgbm as lgb\n",
    "# Configuration\n",
    "# matplotlib.use('nbagg')\n",
    "# plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "sns.set_style('whitegrid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot line figure\n",
    "def line_plot(x_d,y_d,title='',xlabel='',ylabel=''):\n",
    "    plt.plot(x_d, y_d,marker='.')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieId</th>\n",
       "      <th>ReleaseYear</th>\n",
       "      <th>MovieTitle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2003</td>\n",
       "      <td>Dinosaur Planet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2004</td>\n",
       "      <td>Isle of Man TT 2004 Review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1997</td>\n",
       "      <td>Character</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1994</td>\n",
       "      <td>Paula Abdul's Get Up &amp; Dance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2004</td>\n",
       "      <td>The Rise and Fall of ECW</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MovieId  ReleaseYear                    MovieTitle\n",
       "0        1         2003               Dinosaur Planet\n",
       "1        2         2004    Isle of Man TT 2004 Review\n",
       "2        3         1997                     Character\n",
       "3        4         1994  Paula Abdul's Get Up & Dance\n",
       "4        5         2004      The Rise and Fall of ECW"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def handle_bad_lines(line):\n",
    "    fields =[str(field) for field in line]\n",
    "    movie_id = int(fields[0])\n",
    "    release_year = int(fields[1])\n",
    "    combined_title = ''.join(fields[2:]).strip()\n",
    "    return movie_id, release_year, combined_title\n",
    "\n",
    "movie_titles_df = pd.read_csv('movie_titles.csv', names = ['MovieId', 'ReleaseYear', 'MovieTitle'], encoding='ISO-8859-1', engine='python', on_bad_lines=handle_bad_lines)\n",
    "movie_titles_df['ReleaseYear'] = movie_titles_df['ReleaseYear'].astype('Int64')\n",
    "movie_titles_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReleaseYear\n",
       "2004    1436\n",
       "2002    1310\n",
       "2003    1271\n",
       "2000    1234\n",
       "2001    1184\n",
       "        ... \n",
       "1918       2\n",
       "1923       2\n",
       "1914       2\n",
       "1909       1\n",
       "1896       1\n",
       "Name: count, Length: 94, dtype: Int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_titles_df['ReleaseYear'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "movie_titles_df['ReleaseYear'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature binning model\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "kbmodel = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile') # strategies: uniform quantile kmeans\n",
    "\n",
    "result = kbmodel.fit_transform(movie_titles_df[pd.isna(movie_titles_df['ReleaseYear'])==False]['ReleaseYear'].values.reshape(-1, 1))\n",
    "\n",
    "bin_edge = kbmodel.bin_edges_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([1896., 1980., 1994., 1999., 2002., 2005.])], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_titles_df['ReleaseYear_Bin'] = -1\n",
    "movie_titles_df.loc[(movie_titles_df['ReleaseYear']>=bin_edge[0][0])&(movie_titles_df['ReleaseYear']<bin_edge[0][1]),'ReleaseYear_Bin'] = 0\n",
    "movie_titles_df.loc[(movie_titles_df['ReleaseYear']>=bin_edge[0][1])&(movie_titles_df['ReleaseYear']<bin_edge[0][2]),'ReleaseYear_Bin'] = 1\n",
    "movie_titles_df.loc[(movie_titles_df['ReleaseYear']>=bin_edge[0][2])&(movie_titles_df['ReleaseYear']<bin_edge[0][3]),'ReleaseYear_Bin'] = 2\n",
    "movie_titles_df.loc[(movie_titles_df['ReleaseYear']>=bin_edge[0][3])&(movie_titles_df['ReleaseYear']<bin_edge[0][4]),'ReleaseYear_Bin'] = 3\n",
    "#movie_titles_df.loc[(movie_titles_df['m102_sum']>=bin_edge[0][2])&(movie_titles_df['m102_sum']<bin_edge[0][3]),'ReleaseYear_Bin'] = 2\n",
    "movie_titles_df.loc[(movie_titles_df['ReleaseYear']>=bin_edge[0][4]),'ReleaseYear_Bin'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReleaseYear_Bin\n",
       " 4    4529\n",
       " 0    3507\n",
       " 1    3466\n",
       " 3    3383\n",
       " 2    2878\n",
       "-1       7\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_titles_df['ReleaseYear_Bin'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "movie_titles_df['ReleaseYear_Bin'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken : 0:00:00.000998\n"
     ]
    }
   ],
   "source": [
    "# Processing training data\n",
    "start = datetime.now()\n",
    "if not os.path.isfile('data.csv') and not os.path.isfile('featured_train_test_data.csv'):\n",
    "    data = open('data.csv', mode='w')\n",
    "    \n",
    "    row = list()\n",
    "    files = [\n",
    "        'combined_data_1.txt',\n",
    "        'combined_data_2.txt', \n",
    "        'combined_data_3.txt', \n",
    "        'combined_data_4.txt'\n",
    "    ]\n",
    "    for file in files:\n",
    "        print(\"Reading ratings from {}\\n\".format(file))\n",
    "        with open(file) as f:\n",
    "            for line in f: \n",
    "                line = line.strip()\n",
    "                if line.endswith(':'):\n",
    "                    # All below are ratings for this movie, until another movie appears.\n",
    "                    movie_id = line.replace(':', '')\n",
    "                else:\n",
    "                    row = [x for x in line.split(',')]\n",
    "                    row.insert(0, movie_id)\n",
    "                    data.write(','.join(row))\n",
    "                    data.write('\\n')\n",
    "    data.close()\n",
    "print('Time taken :', datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featured_train_test_data.csv File exists\n"
     ]
    }
   ],
   "source": [
    "#feature engineering\n",
    "if  not os.path.isfile('featured_train_test_data.csv'):\n",
    "    # Convert training data to pandas dataframe\n",
    "    df = pd.read_csv('data.csv', sep=',', \n",
    "        names=['movie', 'user', 'rating', 'date'])\n",
    "    df.date = pd.to_datetime(df.date)\n",
    "    #Shuffle the training data to eliminate the influence of time, so that the model doesnâ€™t miss out on learning about new movies.\n",
    "    df = df.sample(frac=1)\n",
    "    df = df.reset_index(drop=True)\n",
    "    # Rename the primary key from movieid to movie, preparing for the next merge\n",
    "    movie_df = movie_titles_df.rename(index=str,inplace=False,columns={'MovieId':'movie'})\n",
    "    movie_df.head()\n",
    "    # adding the movie's ReleaseYear and releaseYaer_BIn\n",
    "    df_t = df.merge(movie_df[['movie','ReleaseYear','ReleaseYear_Bin']],how='left',on='movie')\n",
    "    df_t.head()\n",
    "    # constructing the composite primary key\n",
    "    df_t['movie_user']=df_t['movie'].astype(str)+df_t['user'].astype(str)\n",
    "    df_t.head()\n",
    "    # calculating feature: time difference between watching date and ReleaseYear\n",
    "    df_t['watch_year_dif']=df_t['date'].dt.year-df_t['ReleaseYear']\n",
    "    df_t.head()\n",
    "    # Constructing onehot feature from binning, representing the category of ReleaseYear\n",
    "    cata_cols=['ReleaseYear_Bin']\n",
    "    \n",
    "    one_hot = {'ReleaseYear_Bin': [0,1,2,3,4]}\n",
    "    for col in one_hot:\n",
    "        for val in one_hot[col]:\n",
    "            #print(col)\n",
    "            #print(val)\n",
    "            df_t['hot_' + col + '_' +str(val)] = 1*(df_t[col].values == val)\n",
    "    df = df_t\n",
    "    del df_t\n",
    "    gc.collect()\n",
    "    #The above constructs the features: watch_year_dif: the difference between the viewing time and the release time of the film; \n",
    "    #the onehot feature constructed based on the data binning (5 bins, each bin represents a decade), representing the decade of the movie;\n",
    "else:\n",
    "    print(\"featured_train_test_data.csv File exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct sparse matrix and divide it into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:05.186610\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "if os.path.isfile('train_sparse_matrix.npz'):\n",
    "    train_sparse_matrix = sparse.load_npz('train_sparse_matrix.npz')\n",
    "else: \n",
    "    train_sparse_matrix = sparse.csr_matrix((\n",
    "        df.rating.values, (df.user.values,df.movie.values)\n",
    "    ))\n",
    "    \n",
    "    print('Done. It\\'s shape is : (user, movie) : ',train_sparse_matrix.shape)\n",
    "    sparse.save_npz(\"train_sparse_matrix.npz\", train_sparse_matrix)\n",
    "    \n",
    "print(datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting samples from a sparse matrix\n",
    "def get_sample_sparse_matrix(sparse_matrix, no_users, no_movies, path, verbose = True):\n",
    "    \"\"\"\n",
    "    It will get it from the ''path'' if it is present  or it will create \n",
    "    and store the sampled sparse matrix in the path specified.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get (row, col) and (rating) tuple from sparse_matrix.\n",
    "    row_ind, col_ind, ratings = sparse.find(sparse_matrix)\n",
    "    users = np.unique(row_ind)\n",
    "    movies = np.unique(col_ind)\n",
    "\n",
    "    print(\"Original Matrix: (users, movies) -- ({} {})\".format(len(users), len(movies)))\n",
    "    print(\"Original Matrix: Ratings -- {}\\n\".format(len(ratings)))\n",
    "\n",
    "    # It's just to make sure to get same sample everytime we run this program & pick without replacement\n",
    "    np.random.seed(15)\n",
    "    sample_users = np.random.choice(users, no_users, replace=False)\n",
    "    sample_movies = np.random.choice(movies, no_movies, replace=False)\n",
    "    \n",
    "    # Get the boolean mask of these sampled_items in originl row/col_inds.\n",
    "    mask = np.logical_and(np.isin(row_ind, sample_users), np.isin(col_ind, sample_movies))\n",
    "    \n",
    "    sample_sparse_matrix = sparse.csr_matrix((ratings[mask], (row_ind[mask], col_ind[mask])),\n",
    "         shape=(max(sample_users)+1, max(sample_movies)+1))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Sampled Matrix : (users, movies) -- ({} {})\".format(len(sample_users), len(sample_movies)))\n",
    "        print(\"Sampled Matrix : Ratings --\", format(ratings[mask].shape[0]))\n",
    "\n",
    "    print('Saving it into disk for further usage!')\n",
    "    # save it into disk\n",
    "    sparse.save_npz(path, sample_sparse_matrix)\n",
    "    if verbose:\n",
    "            print('Done')\n",
    "    \n",
    "    return sample_sparse_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting samples - training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is present in your pwd, getting it from disk!\n",
      "Done\n",
      "0:00:00.045710\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "#path = \"/kaggle/working/sample_train_sparse_matrix.npz\"\n",
    "path = \"sample_train_sparse_matrix.npz\"\n",
    "if os.path.isfile(path):\n",
    "    print(\"It is present in your pwd, getting it from disk!\")\n",
    "    # Just get it from the disk instead of computing it\n",
    "    sample_train_sparse_matrix = sparse.load_npz(path)\n",
    "    print(\"Done\")\n",
    "else: \n",
    "    # Get 10k users and 1k movies from available data \n",
    "    sample_train_sparse_matrix = get_sample_sparse_matrix(train_sparse_matrix, no_users=10000, \n",
    "        no_movies=1000, path = path)\n",
    "    #sample_train_sparse_matrix = get_sample_sparse_matrix(train_sparse_matrix, no_users=2000, \n",
    "    #    no_movies=500, path = path)\n",
    "print(datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train_users, sample_train_movies, sample_train_ratings = sparse.find(sample_train_sparse_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     83,     188,     188, ..., 2649067, 2649067, 2649067])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_train_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the average rating score\n",
    "def get_average_ratings(sparse_matrix, of_users):\n",
    "    # Average ratings of user/movies\n",
    "    ax = 1 if of_users else 0 # 1 - User axes,0 - Movie axes\n",
    "    # \".A1\" is for converting Column_Matrix to 1-D numpy array \n",
    "    sum_of_ratings = sparse_matrix.sum(axis=ax).A1\n",
    "    # Boolean matrix of ratings (whether a user rated that movie or not)\n",
    "    is_rated = (sparse_matrix != 0)\n",
    "    # No of ratings for each user/movie\n",
    "    no_of_ratings = is_rated.sum(axis=ax).A1\n",
    "    # max_user and max_movie ids in sparse matrix \n",
    "    u, m = sparse_matrix.shape\n",
    "    # Create a dictonary of users and their average ratings\n",
    "    average_ratings = {i : sum_of_ratings[i] / no_of_ratings[i]\n",
    "         for i in range(u if of_users else m) if no_of_ratings[i] != 0}\n",
    "\n",
    "    # Return that dictionary of average ratings\n",
    "    return average_ratings\n",
    "\n",
    "# Counting the total number of movies watched by a user, and the total number of users being watched by a movie\n",
    "def get_counts(sparse_matrix, of_users):\n",
    "    # \n",
    "    ax = 1 if of_users else 0 # 1 - User axes,0 - Movie axes\n",
    "    # Boolean matrix of ratings (whether a user rated that movie or not)\n",
    "    is_rated = (sparse_matrix != 0)\n",
    "    # No of ratings for each user/movie\n",
    "    no_of_ratings = is_rated.sum(axis=ax).A1\n",
    "    # max_user and max_movie ids in sparse matrix \n",
    "    u, m = sparse_matrix.shape\n",
    "    # Create a dictonary of users or movie counts\n",
    "    counts = {i : no_of_ratings[i]\n",
    "         for i in range(u if of_users else m) if no_of_ratings[i] != 0}\n",
    "\n",
    "    # Return that dictionary of counts\n",
    "    return counts\n",
    "\n",
    "# Count the highest score ever rated by a user and the highest score ever rated by a movie\n",
    "def get_max_ratings(sparse_matrix, of_users):\n",
    "    # \n",
    "    ax = 1 if of_users else 0 # 1 - User axes,0 - Movie axes\n",
    "\n",
    "    # Max rating index for each user/movie\n",
    "    max_indices = sparse_matrix.argmax(axis=ax)\n",
    "    \n",
    "    # max_user and max_movie ids in sparse matrix \n",
    "    u, m = sparse_matrix.shape\n",
    "    # Create a dictonary of users and their max ratings\n",
    "    counts = {i : sparse_matrix[i,max_indices[i,0]] if of_users else sparse_matrix[max_indices[0,i],i]\n",
    "         for i in range(u if of_users else m) }\n",
    "\n",
    "    # Return that dictionary of counts\n",
    "    return counts\n",
    "\n",
    "# Count the lowest score ever rated by a user and the lowest score ever rated by a movie\n",
    "def get_min_ratings(sparse_matrix, of_users):\n",
    "    #\n",
    "    ax = 1 if of_users else 0 # 1 - User axes,0 - Movie axes\n",
    "    # min rating index for each user/movie\n",
    "    min_indices = sparse_matrix.argmin(axis=ax) \n",
    "    # max_user and max_movie ids in sparse matrix \n",
    "    u, m = sparse_matrix.shape\n",
    "    # Create a dictonary of users and their min ratings\n",
    "    counts = {i : sparse_matrix[i,min_indices[i,0]] if of_users else sparse_matrix[min_indices[0,i],i]\n",
    "         for i in range(u if of_users else m) }\n",
    "\n",
    "    # Return that dictionary of counts\n",
    "    return counts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a dictionary to query data by userid or movieid\n",
    "if  not os.path.isfile('featured_train_test_data.csv'):\n",
    "    sample_train_averages = dict()\n",
    "    global_average = sample_train_sparse_matrix.sum()/sample_train_sparse_matrix.count_nonzero()\n",
    "    sample_train_averages['global'] = global_average\n",
    "    # Average rating score of users\n",
    "    sample_train_averages['user'] = get_average_ratings(sample_train_sparse_matrix, of_users=True)\n",
    "    # Average movie rating score\n",
    "    sample_train_averages['movie'] =  get_average_ratings(sample_train_sparse_matrix, of_users=False)\n",
    "    # Number of movie viewings of a user\n",
    "    sample_train_averages['user_watched_'] = get_counts(sample_train_sparse_matrix, of_users=True)\n",
    "    # Watches of a movie\n",
    "    sample_train_averages['movie_watched_'] =  get_counts(sample_train_sparse_matrix, of_users=False)\n",
    "    # The highest score rated by users\n",
    "    sample_train_averages['user_max_rating'] = get_max_ratings(sample_train_sparse_matrix, of_users=True)\n",
    "    # The highest rating a movie has ever received\n",
    "    sample_train_averages['movie_max_rating'] =  get_max_ratings(sample_train_sparse_matrix, of_users=False)\n",
    "    # The lowest score rated by users\n",
    "    sample_train_averages['user_min_rating'] = get_min_ratings(sample_train_sparse_matrix, of_users=True)\n",
    "    # The lowest rating a movie has ever received\n",
    "    sample_train_averages['movie_min_rating'] =  get_min_ratings(sample_train_sparse_matrix, of_users=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.remove(\"featured_reg_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calculating the scores of similar users for movies and the scores of users for similar movies, we construct features and output training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists you don't have to prepare again!\n",
      "0:00:00.002018\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "if os.path.isfile('featured_reg_train.csv'):\n",
    "    print(\"File already exists you don't have to prepare again!\")\n",
    "else:\n",
    "    print('Preparing {} tuples for the dataset'.format(len(sample_train_ratings)))\n",
    "    with open('featured_reg_train.csv', mode='w') as reg_data_file:\n",
    "        count = 0\n",
    "        for (user, movie, rating)  in zip(sample_train_users, sample_train_movies, sample_train_ratings):\n",
    "            st = datetime.now()\n",
    "            # =============================================\n",
    "            # Ratings of \"movie\" by similar users of \"user\"\n",
    "            # =============================================\n",
    "            \n",
    "            # Compute the similar Users of the \"user\"        \n",
    "            user_sim = cosine_similarity(sample_train_sparse_matrix[user], sample_train_sparse_matrix).ravel()\n",
    "            # We are ignoring 'The User' from its similar users\n",
    "            top_sim_users = user_sim.argsort()[::-1][1:] \n",
    "            # Get the ratings of most similar users for this movie\n",
    "            top_ratings = sample_train_sparse_matrix[top_sim_users, movie].toarray().ravel()\n",
    "            # We will make it's length \"5\" by adding movie averages to the rest of the cells.\n",
    "            top_sim_users_ratings = list(top_ratings[top_ratings != 0][:5])\n",
    "            #top_sim_users_ratings.extend([sample_train_averages['movie'][movie]]*(5 - len(top_sim_users_ratings)))\n",
    "            # If the ratings are less than 5, use similar movies' ratings for this user\n",
    "            if len(top_sim_users_ratings) < 5:\n",
    "                # Compute the similar movies of the target movie\n",
    "                movie_sim = cosine_similarity(sample_train_sparse_matrix[:, movie].T, sample_train_sparse_matrix.T).ravel()\n",
    "                # Ignore the target movie itself\n",
    "                top_sim_movies = movie_sim.argsort()[::-1][1:11]  # Top 5 similar movies\n",
    "\n",
    "                # Get the average ratings of the similar movies rated by the current similar user\n",
    "                additional_ratings = []\n",
    "                for sim_movie in top_sim_movies:\n",
    "                    movie_ratings = sample_train_sparse_matrix[top_sim_users, sim_movie].toarray().ravel()\n",
    "                    movie_avg = movie_ratings[movie_ratings != 0].mean() if (movie_ratings != 0).any() else None\n",
    "                    if movie_avg is not None:\n",
    "                        additional_ratings.append(movie_avg)\n",
    "                        if len(top_sim_users_ratings) + len(additional_ratings) >= 5:\n",
    "                            break\n",
    "\n",
    "                # Add the additional ratings to top_sim_users_ratings\n",
    "                top_sim_users_ratings.extend(additional_ratings)\n",
    "\n",
    "            # If still not enough, fill the rest with the movie's global average\n",
    "            if len(top_sim_users_ratings) < 5:\n",
    "                top_sim_users_ratings.extend([sample_train_averages['movie'][movie]] * (5 - len(top_sim_users_ratings)))\n",
    "\n",
    "            # ==============================================\n",
    "            # Ratings by \"user\"  to similar movies of \"movie\"\n",
    "            # ==============================================\n",
    "            \n",
    "            # Compute the similar movies of the \"movie\"        \n",
    "            movie_sim = cosine_similarity(sample_train_sparse_matrix[:,movie].T, sample_train_sparse_matrix.T).ravel()\n",
    "            # We are ignoring 'The Movie' from its similar movies.\n",
    "            top_sim_movies = movie_sim.argsort()[::-1][1:] \n",
    "            # Get the ratings of most similar movie rated by this user\n",
    "            top_ratings = sample_train_sparse_matrix[user, top_sim_movies].toarray().ravel()\n",
    "            # We will make it's length \"5\" by adding user averages to the rest of the cells\n",
    "            top_sim_movies_ratings = list(top_ratings[top_ratings != 0][:5])\n",
    "            #top_sim_movies_ratings.extend([sample_train_averages['user'][user]]*(5-len(top_sim_movies_ratings))) \n",
    "            # If ratings are less than 5, use ratings from similar users for this similar movie\n",
    "            if len(top_sim_movies_ratings) < 5:\n",
    "                # Compute the similar users of the current user\n",
    "                user_sim = cosine_similarity(sample_train_sparse_matrix[user], sample_train_sparse_matrix).ravel()\n",
    "                # Ignore the target user\n",
    "                top_sim_users = user_sim.argsort()[::-1][1:]  # Top similar users\n",
    "\n",
    "                # Get the average rating for the remaining similar movies by similar users\n",
    "                additional_ratings = []\n",
    "                for sim_movie in top_sim_movies:\n",
    "                    # Get ratings of the similar movie by similar users\n",
    "                    similar_user_ratings = sample_train_sparse_matrix[top_sim_users, sim_movie].toarray().ravel()\n",
    "                    movie_avg_rating = similar_user_ratings[similar_user_ratings != 0].mean() if (similar_user_ratings != 0).any() else None\n",
    "                    \n",
    "                    if movie_avg_rating is not None:\n",
    "                        additional_ratings.append(movie_avg_rating)\n",
    "                        if len(top_sim_movies_ratings) + len(additional_ratings) >= 5:\n",
    "                            break\n",
    "\n",
    "                # Add the additional ratings to top_sim_movies_ratings\n",
    "                top_sim_movies_ratings.extend(additional_ratings)\n",
    "\n",
    "            # If still not enough, use the average rating of the similar movie\n",
    "            if len(top_sim_movies_ratings) < 5:\n",
    "                top_sim_movies_ratings.extend([sample_train_averages['movie'][sim_movie]] * (5 - len(top_sim_movies_ratings)))\n",
    "    \n",
    "            # ======================================\n",
    "            # Prepare the row to be stores in a file\n",
    "            # ======================================\n",
    "            \n",
    "            row = list()\n",
    "            row.append(user)\n",
    "            row.append(movie)\n",
    "            # Now add the other features to this data; First Feature\n",
    "            #row.append(sample_train_averages['global']) \n",
    "            # Next 5 features are similar_users \"movie\" ratings\n",
    "            row.extend(top_sim_users_ratings)\n",
    "            # Next 5 features are \"user\" ratings for similar_movies\n",
    "            row.extend(top_sim_movies_ratings)\n",
    "            \n",
    "            # Avg_user rating\n",
    "            #row.append(sample_train_averages['user'][user])\n",
    "            # Avg_movie rating\n",
    "            #row.append(sample_train_averages['movie'][movie])\n",
    "            # Finally, the actual rating of this user-movie pair\n",
    "            row.append(rating)\n",
    "            count = count + 1\n",
    "\n",
    "            # Add rows to the file opened\n",
    "            reg_data_file.write(','.join(map(str, row)))\n",
    "            reg_data_file.write('\\n')        \n",
    "            if (count)%300 == 0:\n",
    "                print(\"Done for {} rows - {}\".format(count, datetime.now() - start))\n",
    "\n",
    "print(datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists you don't have to prepare again!\n",
      "0:00:00.002439\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "if os.path.isfile('featured_reg_train_plus.csv'):\n",
    "    print(\"File already exists you don't have to prepare again!\")\n",
    "else:\n",
    "    print('Preparing {} tuples for the dataset'.format(len(sample_train_ratings)))\n",
    "    with open('featured_reg_train_plus.csv', mode='w') as reg_data_file:\n",
    "        count = 0\n",
    "        for (user, movie, rating)  in zip(sample_train_users, sample_train_movies, sample_train_ratings):\n",
    "            st = datetime.now()\n",
    "            row = list()\n",
    "            row.append(user)\n",
    "            row.append(movie)\n",
    "            # Avg_user rating\n",
    "            row.append(sample_train_averages['user'][user])\n",
    "            # Avg_movie rating\n",
    "            row.append(sample_train_averages['movie'][movie])\n",
    "            row.append(sample_train_averages['user_watched_'][user])\n",
    "            row.append(sample_train_averages['movie_watched_'][movie])\n",
    "            row.append(sample_train_averages['user_max_rating'][user])\n",
    "            row.append(sample_train_averages['movie_max_rating'][movie])\n",
    "            row.append(sample_train_averages['user_min_rating'][user])\n",
    "            row.append(sample_train_averages['movie_min_rating'][movie])\n",
    "            # Finally, the actual rating of this user-movie pair\n",
    "            #row.append(rating)\n",
    "            count = count + 1\n",
    "            #print(row)\n",
    "            #print('----')\n",
    "            # Add rows to the file opened\n",
    "            reg_data_file.write(','.join(map(str, row)))\n",
    "            reg_data_file.write('\\n')        \n",
    "            if (count)%1000 == 0:\n",
    "                print(\"Done for {} rows - {}\".format(count, datetime.now() - start))\n",
    "\n",
    "print(datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct train_test_data\n",
    "if  not os.path.isfile('featured_train_test_data.csv'):\n",
    "    reg_train = pd.read_csv('featured_reg_train.csv', names = ['user', 'movie',  'sur1', \n",
    "        'sur2', 'sur3', 'sur4', 'sur5','smr1', 'smr2', 'smr3', 'smr4', 'smr5', 'rating'], \n",
    "        header=None\n",
    "    )\n",
    "    reg_train_plus = pd.read_csv('featured_reg_train_plus.csv', names = ['user', 'movie', 'user_avg','mov_avg', 'user_watched_', \n",
    "        'movie_watched_', 'user_max_rating', 'movie_max_rating', 'user_min_rating','movie_min_rating'], \n",
    "        header=None\n",
    "    )\n",
    "    #primary key movieid+userid for merge\n",
    "    reg_train['movie_user']=reg_train['movie'].astype(str)+reg_train['user'].astype(str)\n",
    "    reg_train_plus['movie_user']=reg_train_plus['movie'].astype(str)+reg_train_plus['user'].astype(str)\n",
    "    #merge oper\n",
    "    reg_train_t = reg_train.merge(reg_train_plus[['movie_user','user_avg','mov_avg', 'user_watched_', \n",
    "        'movie_watched_', 'user_max_rating', 'movie_max_rating']],how='left',on='movie_user')\n",
    "    #df is big,'isin' oper lessen it\n",
    "    df_1 = df[df['movie_user'].isin(reg_train_t['movie_user'].values)]\n",
    "    #merge oper to extend movie's features\n",
    "    reg_train_t = reg_train_t.merge(df_1[['movie_user','watch_year_dif','hot_ReleaseYear_Bin_0', 'hot_ReleaseYear_Bin_1', \n",
    "        'hot_ReleaseYear_Bin_2', 'hot_ReleaseYear_Bin_3', 'hot_ReleaseYear_Bin_4']],how='left',on='movie_user')\n",
    "    reg_train = reg_train_t\n",
    "    #drop non rating samples\n",
    "    reg_train_t = reg_train[pd.isna(reg_train['rating'])==False].reset_index(drop=True)\n",
    "    #drop primary key\n",
    "    reg_train_t = reg_train_t.drop(['movie_user'], axis=1)\n",
    "    #output file\n",
    "    reg_train_t.to_csv('featured_train_test_data.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>movie</th>\n",
       "      <th>sur1</th>\n",
       "      <th>sur2</th>\n",
       "      <th>sur3</th>\n",
       "      <th>sur4</th>\n",
       "      <th>sur5</th>\n",
       "      <th>smr1</th>\n",
       "      <th>smr2</th>\n",
       "      <th>smr3</th>\n",
       "      <th>...</th>\n",
       "      <th>user_watched_</th>\n",
       "      <th>movie_watched_</th>\n",
       "      <th>user_max_rating</th>\n",
       "      <th>movie_max_rating</th>\n",
       "      <th>watch_year_dif</th>\n",
       "      <th>hot_ReleaseYear_Bin_0</th>\n",
       "      <th>hot_ReleaseYear_Bin_1</th>\n",
       "      <th>hot_ReleaseYear_Bin_2</th>\n",
       "      <th>hot_ReleaseYear_Bin_3</th>\n",
       "      <th>hot_ReleaseYear_Bin_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83</td>\n",
       "      <td>15582</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2917</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>188</td>\n",
       "      <td>1102</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>1243</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>188</td>\n",
       "      <td>1905</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>3255</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>188</td>\n",
       "      <td>2122</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>1802</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>188</td>\n",
       "      <td>3624</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>2337</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  movie  sur1  sur2  sur3  sur4  sur5  smr1  smr2  smr3  ...  \\\n",
       "0    83  15582   5.0   3.0   4.0   4.0   2.0   4.0   4.0   4.0  ...   \n",
       "1   188   1102   5.0   4.0   4.0   5.0   5.0   4.0   4.0   3.0  ...   \n",
       "2   188   1905   5.0   5.0   4.0   3.0   5.0   3.0   4.0   3.0  ...   \n",
       "3   188   2122   4.0   3.0   1.0   3.0   1.0   4.0   4.0   3.0  ...   \n",
       "4   188   3624   5.0   4.0   4.0   5.0   4.0   4.0   3.0   3.0  ...   \n",
       "\n",
       "   user_watched_  movie_watched_  user_max_rating  movie_max_rating  \\\n",
       "0              1            2917                4                 5   \n",
       "1             21            1243                4                 5   \n",
       "2             21            3255                4                 5   \n",
       "3             21            1802                4                 5   \n",
       "4             21            2337                4                 5   \n",
       "\n",
       "   watch_year_dif  hot_ReleaseYear_Bin_0  hot_ReleaseYear_Bin_1  \\\n",
       "0               1                      0                      0   \n",
       "1               3                      0                      0   \n",
       "2               1                      0                      0   \n",
       "3               5                      0                      0   \n",
       "4               1                      0                      0   \n",
       "\n",
       "   hot_ReleaseYear_Bin_2  hot_ReleaseYear_Bin_3  hot_ReleaseYear_Bin_4  \n",
       "0                      0                      0                      1  \n",
       "1                      0                      1                      0  \n",
       "2                      0                      0                      1  \n",
       "3                      0                      1                      0  \n",
       "4                      0                      0                      1  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_data = pd.read_csv('featured_train_test_data.csv')\n",
    "train_test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>movie</th>\n",
       "      <th>sur1</th>\n",
       "      <th>sur2</th>\n",
       "      <th>sur3</th>\n",
       "      <th>sur4</th>\n",
       "      <th>sur5</th>\n",
       "      <th>smr1</th>\n",
       "      <th>smr2</th>\n",
       "      <th>smr3</th>\n",
       "      <th>...</th>\n",
       "      <th>user_watched_</th>\n",
       "      <th>movie_watched_</th>\n",
       "      <th>user_max_rating</th>\n",
       "      <th>movie_max_rating</th>\n",
       "      <th>watch_year_dif</th>\n",
       "      <th>hot_ReleaseYear_Bin_0</th>\n",
       "      <th>hot_ReleaseYear_Bin_1</th>\n",
       "      <th>hot_ReleaseYear_Bin_2</th>\n",
       "      <th>hot_ReleaseYear_Bin_3</th>\n",
       "      <th>hot_ReleaseYear_Bin_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83</td>\n",
       "      <td>15582</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2917</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>188</td>\n",
       "      <td>1102</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>1243</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>188</td>\n",
       "      <td>1905</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>3255</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>188</td>\n",
       "      <td>2122</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>1802</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>188</td>\n",
       "      <td>3624</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>2337</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97391</th>\n",
       "      <td>2649067</td>\n",
       "      <td>14590</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>1412</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97392</th>\n",
       "      <td>2649067</td>\n",
       "      <td>14712</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>2151</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97393</th>\n",
       "      <td>2649067</td>\n",
       "      <td>16063</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>762</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97394</th>\n",
       "      <td>2649067</td>\n",
       "      <td>16390</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>1052</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97395</th>\n",
       "      <td>2649067</td>\n",
       "      <td>17328</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>961</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97396 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          user  movie  sur1  sur2  sur3  sur4  sur5  smr1  smr2  smr3  ...  \\\n",
       "0           83  15582   5.0   3.0   4.0   4.0   2.0   4.0   4.0   4.0  ...   \n",
       "1          188   1102   5.0   4.0   4.0   5.0   5.0   4.0   4.0   3.0  ...   \n",
       "2          188   1905   5.0   5.0   4.0   3.0   5.0   3.0   4.0   3.0  ...   \n",
       "3          188   2122   4.0   3.0   1.0   3.0   1.0   4.0   4.0   3.0  ...   \n",
       "4          188   3624   5.0   4.0   4.0   5.0   4.0   4.0   3.0   3.0  ...   \n",
       "...        ...    ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n",
       "97391  2649067  14590   3.0   5.0   4.0   4.0   3.0   4.0   5.0   5.0  ...   \n",
       "97392  2649067  14712   3.0   3.0   4.0   4.0   4.0   4.0   4.0   5.0  ...   \n",
       "97393  2649067  16063   3.0   5.0   5.0   5.0   4.0   3.0   5.0   5.0  ...   \n",
       "97394  2649067  16390   1.0   4.0   3.0   4.0   3.0   5.0   5.0   3.0  ...   \n",
       "97395  2649067  17328   4.0   5.0   5.0   4.0   4.0   5.0   5.0   5.0  ...   \n",
       "\n",
       "       user_watched_  movie_watched_  user_max_rating  movie_max_rating  \\\n",
       "0                  1            2917                4                 5   \n",
       "1                 21            1243                4                 5   \n",
       "2                 21            3255                4                 5   \n",
       "3                 21            1802                4                 5   \n",
       "4                 21            2337                4                 5   \n",
       "...              ...             ...              ...               ...   \n",
       "97391             24            1412                5                 5   \n",
       "97392             24            2151                5                 5   \n",
       "97393             24             762                5                 5   \n",
       "97394             24            1052                5                 5   \n",
       "97395             24             961                5                 5   \n",
       "\n",
       "       watch_year_dif  hot_ReleaseYear_Bin_0  hot_ReleaseYear_Bin_1  \\\n",
       "0                   1                      0                      0   \n",
       "1                   3                      0                      0   \n",
       "2                   1                      0                      0   \n",
       "3                   5                      0                      0   \n",
       "4                   1                      0                      0   \n",
       "...               ...                    ...                    ...   \n",
       "97391               9                      0                      0   \n",
       "97392               3                      0                      0   \n",
       "97393              16                      0                      1   \n",
       "97394               2                      0                      0   \n",
       "97395               2                      0                      0   \n",
       "\n",
       "       hot_ReleaseYear_Bin_2  hot_ReleaseYear_Bin_3  hot_ReleaseYear_Bin_4  \n",
       "0                          0                      0                      1  \n",
       "1                          0                      1                      0  \n",
       "2                          0                      0                      1  \n",
       "3                          0                      1                      0  \n",
       "4                          0                      0                      1  \n",
       "...                      ...                    ...                    ...  \n",
       "97391                      1                      0                      0  \n",
       "97392                      0                      1                      0  \n",
       "97393                      0                      0                      0  \n",
       "97394                      0                      0                      1  \n",
       "97395                      0                      0                      1  \n",
       "\n",
       "[97396 rows x 25 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#90% for trainï¼Œ10% for test \n",
    "train_test_sep = int(len(train_test_data)*0.9)\n",
    "# Prepare train data\n",
    "x_train = train_test_data[:train_test_sep].drop(['user', 'movie', 'rating',], axis=1)\n",
    "y_train = train_test_data[:train_test_sep]['rating']\n",
    "\n",
    "# Prepare test data\n",
    "x_test = train_test_data[train_test_sep:].drop(['user', 'movie', 'rating'], axis=1)\n",
    "y_test = train_test_data[train_test_sep:]['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sur1</th>\n",
       "      <th>sur2</th>\n",
       "      <th>sur3</th>\n",
       "      <th>sur4</th>\n",
       "      <th>sur5</th>\n",
       "      <th>smr1</th>\n",
       "      <th>smr2</th>\n",
       "      <th>smr3</th>\n",
       "      <th>smr4</th>\n",
       "      <th>smr5</th>\n",
       "      <th>...</th>\n",
       "      <th>user_watched_</th>\n",
       "      <th>movie_watched_</th>\n",
       "      <th>user_max_rating</th>\n",
       "      <th>movie_max_rating</th>\n",
       "      <th>watch_year_dif</th>\n",
       "      <th>hot_ReleaseYear_Bin_0</th>\n",
       "      <th>hot_ReleaseYear_Bin_1</th>\n",
       "      <th>hot_ReleaseYear_Bin_2</th>\n",
       "      <th>hot_ReleaseYear_Bin_3</th>\n",
       "      <th>hot_ReleaseYear_Bin_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2917</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>1243</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>3255</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>1802</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>2337</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87651</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>76</td>\n",
       "      <td>351</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87652</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>76</td>\n",
       "      <td>674</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87653</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>76</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87654</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>76</td>\n",
       "      <td>653</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87655</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>76</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87656 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sur1  sur2  sur3  sur4  sur5  smr1  smr2  smr3  smr4  smr5  ...  \\\n",
       "0       5.0   3.0   4.0   4.0   2.0   4.0   4.0   4.0   4.0   4.0  ...   \n",
       "1       5.0   4.0   4.0   5.0   5.0   4.0   4.0   3.0   4.0   3.0  ...   \n",
       "2       5.0   5.0   4.0   3.0   5.0   3.0   4.0   3.0   3.0   4.0  ...   \n",
       "3       4.0   3.0   1.0   3.0   1.0   4.0   4.0   3.0   4.0   3.0  ...   \n",
       "4       5.0   4.0   4.0   5.0   4.0   4.0   3.0   3.0   4.0   4.0  ...   \n",
       "...     ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n",
       "87651   3.0   3.0   4.0   3.0   4.0   4.0   4.0   3.0   3.0   4.0  ...   \n",
       "87652   4.0   4.0   4.0   4.0   3.0   3.0   3.0   4.0   3.0   3.0  ...   \n",
       "87653   4.0   4.0   4.0   4.0   4.0   2.0   3.0   3.0   4.0   2.0  ...   \n",
       "87654   3.0   4.0   4.0   3.0   3.0   3.0   3.0   4.0   5.0   4.0  ...   \n",
       "87655   4.0   3.0   5.0   1.0   3.0   5.0   2.0   2.0   4.0   3.0  ...   \n",
       "\n",
       "       user_watched_  movie_watched_  user_max_rating  movie_max_rating  \\\n",
       "0                  1            2917                4                 5   \n",
       "1                 21            1243                4                 5   \n",
       "2                 21            3255                4                 5   \n",
       "3                 21            1802                4                 5   \n",
       "4                 21            2337                4                 5   \n",
       "...              ...             ...              ...               ...   \n",
       "87651             76             351                5                 5   \n",
       "87652             76             674                5                 5   \n",
       "87653             76              20                5                 5   \n",
       "87654             76             653                5                 5   \n",
       "87655             76              18                5                 5   \n",
       "\n",
       "       watch_year_dif  hot_ReleaseYear_Bin_0  hot_ReleaseYear_Bin_1  \\\n",
       "0                   1                      0                      0   \n",
       "1                   3                      0                      0   \n",
       "2                   1                      0                      0   \n",
       "3                   5                      0                      0   \n",
       "4                   1                      0                      0   \n",
       "...               ...                    ...                    ...   \n",
       "87651              54                      1                      0   \n",
       "87652              26                      1                      0   \n",
       "87653              49                      1                      0   \n",
       "87654               1                      0                      0   \n",
       "87655               8                      0                      0   \n",
       "\n",
       "       hot_ReleaseYear_Bin_2  hot_ReleaseYear_Bin_3  hot_ReleaseYear_Bin_4  \n",
       "0                          0                      0                      1  \n",
       "1                          0                      1                      0  \n",
       "2                          0                      0                      1  \n",
       "3                          0                      1                      0  \n",
       "4                          0                      0                      1  \n",
       "...                      ...                    ...                    ...  \n",
       "87651                      0                      0                      0  \n",
       "87652                      0                      0                      0  \n",
       "87653                      0                      0                      0  \n",
       "87654                      0                      0                      1  \n",
       "87655                      1                      0                      0  \n",
       "\n",
       "[87656 rows x 22 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sur1</th>\n",
       "      <th>sur2</th>\n",
       "      <th>sur3</th>\n",
       "      <th>sur4</th>\n",
       "      <th>sur5</th>\n",
       "      <th>smr1</th>\n",
       "      <th>smr2</th>\n",
       "      <th>smr3</th>\n",
       "      <th>smr4</th>\n",
       "      <th>smr5</th>\n",
       "      <th>...</th>\n",
       "      <th>user_watched_</th>\n",
       "      <th>movie_watched_</th>\n",
       "      <th>user_max_rating</th>\n",
       "      <th>movie_max_rating</th>\n",
       "      <th>watch_year_dif</th>\n",
       "      <th>hot_ReleaseYear_Bin_0</th>\n",
       "      <th>hot_ReleaseYear_Bin_1</th>\n",
       "      <th>hot_ReleaseYear_Bin_2</th>\n",
       "      <th>hot_ReleaseYear_Bin_3</th>\n",
       "      <th>hot_ReleaseYear_Bin_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>87656</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>76</td>\n",
       "      <td>1057</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87657</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>76</td>\n",
       "      <td>442</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87658</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>76</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87659</th>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>76</td>\n",
       "      <td>62</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87660</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>76</td>\n",
       "      <td>2539</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97391</th>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>1412</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97392</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>2151</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97393</th>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>762</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97394</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>1052</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97395</th>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>961</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9740 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sur1  sur2  sur3  sur4  sur5  smr1  smr2  smr3  smr4  smr5  ...  \\\n",
       "87656   4.0   4.0   4.0   3.0   2.0   3.0   4.0   3.0   5.0   4.0  ...   \n",
       "87657   4.0   2.0   4.0   3.0   3.0   3.0   3.0   3.0   2.0   2.0  ...   \n",
       "87658   4.0   3.0   3.0   5.0   5.0   2.0   4.0   5.0   2.0   3.0  ...   \n",
       "87659   4.0   5.0   4.0   2.0   5.0   3.0   3.0   4.0   2.0   2.0  ...   \n",
       "87660   2.0   2.0   2.0   5.0   3.0   4.0   3.0   3.0   2.0   5.0  ...   \n",
       "...     ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n",
       "97391   3.0   5.0   4.0   4.0   3.0   4.0   5.0   5.0   4.0   4.0  ...   \n",
       "97392   3.0   3.0   4.0   4.0   4.0   4.0   4.0   5.0   5.0   5.0  ...   \n",
       "97393   3.0   5.0   5.0   5.0   4.0   3.0   5.0   5.0   5.0   4.0  ...   \n",
       "97394   1.0   4.0   3.0   4.0   3.0   5.0   5.0   3.0   5.0   4.0  ...   \n",
       "97395   4.0   5.0   5.0   4.0   4.0   5.0   5.0   5.0   4.0   4.0  ...   \n",
       "\n",
       "       user_watched_  movie_watched_  user_max_rating  movie_max_rating  \\\n",
       "87656             76            1057                5                 5   \n",
       "87657             76             442                5                 5   \n",
       "87658             76              18                5                 5   \n",
       "87659             76              62                5                 5   \n",
       "87660             76            2539                5                 5   \n",
       "...              ...             ...              ...               ...   \n",
       "97391             24            1412                5                 5   \n",
       "97392             24            2151                5                 5   \n",
       "97393             24             762                5                 5   \n",
       "97394             24            1052                5                 5   \n",
       "97395             24             961                5                 5   \n",
       "\n",
       "       watch_year_dif  hot_ReleaseYear_Bin_0  hot_ReleaseYear_Bin_1  \\\n",
       "87656              17                      0                      1   \n",
       "87657               4                      0                      0   \n",
       "87658               3                      0                      0   \n",
       "87659              45                      1                      0   \n",
       "87660               2                      0                      0   \n",
       "...               ...                    ...                    ...   \n",
       "97391               9                      0                      0   \n",
       "97392               3                      0                      0   \n",
       "97393              16                      0                      1   \n",
       "97394               2                      0                      0   \n",
       "97395               2                      0                      0   \n",
       "\n",
       "       hot_ReleaseYear_Bin_2  hot_ReleaseYear_Bin_3  hot_ReleaseYear_Bin_4  \n",
       "87656                      0                      0                      0  \n",
       "87657                      0                      1                      0  \n",
       "87658                      0                      0                      1  \n",
       "87659                      0                      0                      0  \n",
       "87660                      0                      0                      1  \n",
       "...                      ...                    ...                    ...  \n",
       "97391                      1                      0                      0  \n",
       "97392                      0                      1                      0  \n",
       "97393                      0                      0                      0  \n",
       "97394                      0                      0                      1  \n",
       "97395                      0                      0                      1  \n",
       "\n",
       "[9740 rows x 22 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 87656 entries, 0 to 87655\n",
      "Data columns (total 22 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   sur1                   87656 non-null  float64\n",
      " 1   sur2                   87656 non-null  float64\n",
      " 2   sur3                   87656 non-null  float64\n",
      " 3   sur4                   87656 non-null  float64\n",
      " 4   sur5                   87656 non-null  float64\n",
      " 5   smr1                   87656 non-null  float64\n",
      " 6   smr2                   87656 non-null  float64\n",
      " 7   smr3                   87656 non-null  float64\n",
      " 8   smr4                   87656 non-null  float64\n",
      " 9   smr5                   87656 non-null  float64\n",
      " 10  user_avg               87656 non-null  float64\n",
      " 11  mov_avg                87656 non-null  float64\n",
      " 12  user_watched_          87656 non-null  int64  \n",
      " 13  movie_watched_         87656 non-null  int64  \n",
      " 14  user_max_rating        87656 non-null  int64  \n",
      " 15  movie_max_rating       87656 non-null  int64  \n",
      " 16  watch_year_dif         87656 non-null  int64  \n",
      " 17  hot_ReleaseYear_Bin_0  87656 non-null  int64  \n",
      " 18  hot_ReleaseYear_Bin_1  87656 non-null  int64  \n",
      " 19  hot_ReleaseYear_Bin_2  87656 non-null  int64  \n",
      " 20  hot_ReleaseYear_Bin_3  87656 non-null  int64  \n",
      " 21  hot_ReleaseYear_Bin_4  87656 non-null  int64  \n",
      "dtypes: float64(12), int64(10)\n",
      "memory usage: 14.7 MB\n"
     ]
    }
   ],
   "source": [
    "x_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#input features num\n",
    "x_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Neural Network (DNN) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "#Define DNN model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "class DNNRegressionModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DNNRegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # first layer\n",
    "        self.relu = nn.ReLU()  # relu func\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)  # second layer\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "x_train['watch_year_dif'] = x_train['watch_year_dif'].astype(np.int32)\n",
    "X_train = torch.tensor(x_train.values, dtype=torch.float32)\n",
    "y_target = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)  # label\n",
    "dataset = TensorDataset(X_train, y_target)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "#test data\n",
    "x_test['watch_year_dif'] = x_test['watch_year_dif'].astype(np.int32)\n",
    "x_test_tensor = torch.tensor(x_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile('dnn_model_best.pth'):\n",
    "    #find the best hidden_size parameter\n",
    "    hid_size_list=[8,16,32,64,128]\n",
    "    rmse_list=[]\n",
    "    for hi in hid_size_list:\n",
    "        print(f'parameter : {hi}')\n",
    "        # set parameters\n",
    "        input_size = x_train.shape[1]  # input feature num\n",
    "        hidden_size = hi  # hidden nodes\n",
    "        output_size = 1  # output size\n",
    "        learning_rate = 0.001  # learning rate\n",
    "        num_epochs = 100  # train epochs\n",
    "        # init model\n",
    "        model_dn = DNNRegressionModel(input_size, hidden_size, output_size)\n",
    "        # define loss function and optimizer\n",
    "        criterion = nn.MSELoss()  # mse loss func\n",
    "        optimizer = optim.Adam(model_dn.parameters(), lr=learning_rate)  # Adam optimizer\n",
    "        # train the model\n",
    "        for epoch in range(num_epochs):\n",
    "            for data, target in dataloader:   \n",
    "                output = model_dn(data)\n",
    "                loss = criterion(output, target)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}')\n",
    "        model_dn.eval()  \n",
    "        with torch.no_grad():  \n",
    "            predictions = model_dn(x_test_tensor)  # predict  \n",
    "        # calculate test RMSE\n",
    "        rmse = np.sqrt(mean_squared_error(predictions,y_test_tensor))\n",
    "        rmse_list.append(rmse)\n",
    "        print(f'DNN Test RMSE: {rmse.item():.4f}')\n",
    "    line_plot(hid_size_list,rmse_list,'Test RMSE vs hidden_size','hidden_size','RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile('dnn_model_best.pth'):\n",
    "    #find the best learning_rate parameter\n",
    "    learning_rate_list=[0.1,0.01,0.001,0.0001]\n",
    "    rmse_list=[]\n",
    "    for lra in learning_rate_list:\n",
    "        print(f'parameter : {lra}')\n",
    "        # set parameters\n",
    "        input_size = x_train.shape[1]  # input feature num\n",
    "        hidden_size = 32  # hidden nodes\n",
    "        output_size = 1  # output size\n",
    "        learning_rate = lra  # learning rate\n",
    "        num_epochs = 100  # train epochs\n",
    "        # init model\n",
    "        model_dn = DNNRegressionModel(input_size, hidden_size, output_size)\n",
    "        # define loss function and optimizer\n",
    "        criterion = nn.MSELoss()  # mse loss func\n",
    "        optimizer = optim.Adam(model_dn.parameters(), lr=learning_rate)  # Adam optimizer\n",
    "        # train the model\n",
    "        for epoch in range(num_epochs):\n",
    "            for data, target in dataloader:   \n",
    "                output = model_dn(data)\n",
    "                loss = criterion(output, target)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}')\n",
    "        model_dn.eval()  \n",
    "        with torch.no_grad():  \n",
    "            predictions = model_dn(x_test_tensor)  # predict  \n",
    "        # calculate test RMSE\n",
    "        rmse = np.sqrt(mean_squared_error(predictions,y_test_tensor))\n",
    "        rmse_list.append(rmse)\n",
    "        print(f'DNN Test RMSE: {rmse.item():.4f}')\n",
    "    line_plot(learning_rate_list,rmse_list,'DNN Test RMSE vs learning_rate','learning_rate','RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rmse_list[1.0943245, 0.8795845, 0.8058949, 0.86936843]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retraining is not need!\n"
     ]
    }
   ],
   "source": [
    "#train with best parameter\n",
    "if  not os.path.isfile('dnn_model_best.pth'):\n",
    "    # set parameters\n",
    "    input_size = x_train.shape[1]  # input feature num\n",
    "    hidden_size = 64  # hidden nodes\n",
    "    output_size = 1  # output size\n",
    "    learning_rate = 0.001  # learning rate\n",
    "    num_epochs = 1000  # train epochs\n",
    "    # train data\n",
    "    x_train['watch_year_dif'] = x_train['watch_year_dif'].astype(np.int32)\n",
    "    X_train = torch.tensor(x_train.values, dtype=torch.float32)\n",
    "    y_target = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)  # label\n",
    "    dataset = TensorDataset(X_train, y_target)\n",
    "    dataloader = DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "    # init model\n",
    "    model = DNNRegressionModel(input_size, hidden_size, output_size)\n",
    "    # define loss function and optimizer\n",
    "    criterion = nn.MSELoss()  # mse loss func\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Adam optimizer\n",
    "    # train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        for data, target in dataloader:   \n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}')\n",
    "    torch.save(model.state_dict(), \"dnn_model_best.pth\")\n",
    "else:\n",
    "    print('retraining is not need!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN Test RMSE: 0.8035\n"
     ]
    }
   ],
   "source": [
    "# test the dnn model\n",
    "x_test['watch_year_dif'] = x_test['watch_year_dif'].astype(np.int32)\n",
    "x_test_tensor = torch.tensor(x_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
    "if os.path.isfile('dnn_model_best.pth'):\n",
    "    model_dnn = DNNRegressionModel(22,64,1).to(device)\n",
    "    model_dnn.load_state_dict(torch.load(\"dnn_model_best.pth\", weights_only=True))\n",
    "\n",
    "    model_dnn.eval()  \n",
    "    with torch.no_grad():  \n",
    "        predictions = model_dnn(x_test_tensor)  # predict\n",
    "    \n",
    "    # calculate RMSE\n",
    "    #rmse = criterion(predictions, y_test_tensor)\n",
    "    rmse = np.sqrt(mean_squared_error(predictions,y_test_tensor))\n",
    "    print(f'DNN Test RMSE: {rmse.item():.4f}')\n",
    "else:\n",
    "    model.eval()  \n",
    "    with torch.no_grad():  \n",
    "        predictions = model(x_test_tensor)  # predict  \n",
    "    # calculate RMSE\n",
    "    #rmse = criterion(predictions, y_test_tensor)\n",
    "    rmse = np.sqrt(mean_squared_error(predictions,y_test_tensor))\n",
    "    print(f'DNN Test RMSE: {rmse.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 87656 entries, 0 to 87655\n",
      "Data columns (total 22 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   sur1                   87656 non-null  float64\n",
      " 1   sur2                   87656 non-null  float64\n",
      " 2   sur3                   87656 non-null  float64\n",
      " 3   sur4                   87656 non-null  float64\n",
      " 4   sur5                   87656 non-null  float64\n",
      " 5   smr1                   87656 non-null  float64\n",
      " 6   smr2                   87656 non-null  float64\n",
      " 7   smr3                   87656 non-null  float64\n",
      " 8   smr4                   87656 non-null  float64\n",
      " 9   smr5                   87656 non-null  float64\n",
      " 10  user_avg               87656 non-null  float64\n",
      " 11  mov_avg                87656 non-null  float64\n",
      " 12  user_watched_          87656 non-null  int64  \n",
      " 13  movie_watched_         87656 non-null  int64  \n",
      " 14  user_max_rating        87656 non-null  int64  \n",
      " 15  movie_max_rating       87656 non-null  int64  \n",
      " 16  watch_year_dif         87656 non-null  int32  \n",
      " 17  hot_ReleaseYear_Bin_0  87656 non-null  int64  \n",
      " 18  hot_ReleaseYear_Bin_1  87656 non-null  int64  \n",
      " 19  hot_ReleaseYear_Bin_2  87656 non-null  int64  \n",
      " 20  hot_ReleaseYear_Bin_3  87656 non-null  int64  \n",
      " 21  hot_ReleaseYear_Bin_4  87656 non-null  int64  \n",
      "dtypes: float64(12), int32(1), int64(9)\n",
      "memory usage: 14.4 MB\n"
     ]
    }
   ],
   "source": [
    "x_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restricted Boltzmann machine (RBM) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RBM model\n",
    "class SupervisedRBMRegression(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(SupervisedRBMRegression, self).__init__()\n",
    "        self.W = nn.Parameter(torch.randn(input_size, hidden_size) * 0.01)\n",
    "        self.vbias = nn.Parameter(torch.zeros(input_size))\n",
    "        self.hbias = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.regressor = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # forward propagate RBM\n",
    "        hprobs = torch.sigmoid(torch.matmul(x, self.W) + self.hbias)\n",
    "        hsample = torch.bernoulli(hprobs)\n",
    "        # Use the hidden layer output of RBM for regression\n",
    "        regression_output = self.regressor(hsample)\n",
    "        return regression_output\n",
    "# Define RMSE loss function\n",
    "def rmse_loss(output, target):\n",
    "    return torch.sqrt(nn.MSELoss()(output, target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#find the best batch_size parameter\n",
    "if not os.path.isfile('rbm_model.pth'):\n",
    "    batch_size_list=[16,32,64,128]\n",
    "    rmse_list=[]\n",
    "    for lra in batch_size_list:\n",
    "        print(f'parameter : {lra}')\n",
    "        # set parameters\n",
    "        input_size = X_train.shape[1]\n",
    "        hidden_size = 8  # You can adjust the size of the hidden layers as needed\n",
    "        \n",
    "        # Instantiate a model\n",
    "        model_rbm = SupervisedRBMRegression(input_size, hidden_size)\n",
    "        # Assume target is the target value tensor\n",
    "        target = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "        # Create a data loader\n",
    "        dataset = TensorDataset(X_train, target)\n",
    "        dataloader = DataLoader(dataset, batch_size=lra, shuffle=True)\n",
    "        \n",
    "        # Define an optimizer\n",
    "        optimizer = optim.Adam(model_rbm.parameters(), lr=0.001)\n",
    "        # train the model\n",
    "        num_epochs = 100\n",
    "        for epoch in range(num_epochs):\n",
    "            for data, target in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                output = model_rbm(data)\n",
    "                loss = rmse_loss(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}')\n",
    "        model_rbm.eval()  \n",
    "        with torch.no_grad():  \n",
    "            predictions = model_rbm(x_test_tensor)  # predict  \n",
    "        # calculate test RMSE\n",
    "        rmse = np.sqrt(mean_squared_error(predictions,y_test_tensor))\n",
    "        rmse_list.append(rmse)\n",
    "        print(f'DNN Test RMSE: {rmse.item():.4f}')\n",
    "    line_plot(batch_size_list,rmse_list,'RBM Test RMSE vs batch_size','batch_size','RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rmse_list[1.0940537, 1.0904002, 1.0930872, 1.0919379]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#find the best hidden_size parameter\n",
    "if not os.path.isfile('rbm_model.pth'):\n",
    "    hidden_size_list=[8,16,32,64]\n",
    "    rmse_list=[]\n",
    "    for lra in hidden_size_list:\n",
    "        print(f'parameter : {lra}')\n",
    "        # set parameters\n",
    "        input_size = X_train.shape[1]\n",
    "        hidden_size = lra  # You can adjust the size of the hidden layers as needed\n",
    "        \n",
    "        # Instantiate a model\n",
    "        model_rbm = SupervisedRBMRegression(input_size, hidden_size)\n",
    "        # Assume target is the target value tensor\n",
    "        target = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "        # Create a data loader\n",
    "        dataset = TensorDataset(X_train, target)\n",
    "        dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "        \n",
    "        # Define an optimizer\n",
    "        optimizer = optim.Adam(model_rbm.parameters(), lr=0.001)\n",
    "        # train the model\n",
    "        num_epochs = 100\n",
    "        for epoch in range(num_epochs):\n",
    "            for data, target in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                output = model_rbm(data)\n",
    "                loss = rmse_loss(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}')\n",
    "        model_rbm.eval()  \n",
    "        with torch.no_grad():  \n",
    "            predictions = model_rbm(x_test_tensor)  # predict  \n",
    "        # calculate test RMSE\n",
    "        rmse = np.sqrt(mean_squared_error(predictions,y_test_tensor))\n",
    "        rmse_list.append(rmse)\n",
    "        print(f'DNN Test RMSE: {rmse.item():.4f}')\n",
    "    line_plot(hidden_size_list,rmse_list,'RBM Test RMSE vs hidden_size','hidden_size','RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rmse_list[1.0936692, 1.0922198, 1.0935733, 1.0963564]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([87656, 22])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train with best parameter\n",
    "if  not os.path.isfile('rbm_model.pth'):\n",
    "    # Assume that the size of the input feature is input_size and the size of the hidden layer is hidden_size\n",
    "    input_size = X_train.shape[1]\n",
    "    hidden_size = 16  # You can adjust the size of the hidden layers as needed\n",
    "    \n",
    "    # Instantiate a model\n",
    "    model = SupervisedRBMRegression(input_size, hidden_size)\n",
    "    \n",
    "    # Assume target is the target value tensor\n",
    "    target = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)  # Assume the target values are in the last column of the CSV file\n",
    "    # Create a data loader\n",
    "    dataset = TensorDataset(X_train, target)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    # Define an optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    # Train model\n",
    "    best_loss= 9999\n",
    "    best_model_state_dict = None\n",
    "    num_epochs = 1000\n",
    "    for epoch in range(num_epochs):\n",
    "        for data, target in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = rmse_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}')\n",
    "    torch.save(model.state_dict(), \"rbm_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBM Test RMSE: 1.0941\n"
     ]
    }
   ],
   "source": [
    "# test the rbm model\n",
    "x_test['watch_year_dif'] = x_test['watch_year_dif'].astype(np.int32)\n",
    "x_test_tensor = torch.tensor(x_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
    "if os.path.isfile('rbm_model.pth'):\n",
    "    #load rbm_model from disk\n",
    "    model_rbm = SupervisedRBMRegression(22,8).to(device)\n",
    "    model_rbm.load_state_dict(torch.load(\"rbm_model.pth\", weights_only=True))\n",
    "    model_rbm.eval()  \n",
    "    with torch.no_grad():  \n",
    "        predictions = model_rbm(x_test_tensor)  # predict\n",
    "    \n",
    "    # calculate RMSE\n",
    "    #rmse = criterion(predictions, y_test_tensor)\n",
    "    rmse = np.sqrt(mean_squared_error(predictions,y_test_tensor))\n",
    "    print(f'RBM Test RMSE: {rmse.item():.4f}')\n",
    "else:\n",
    "    model.eval()  \n",
    "    with torch.no_grad():  \n",
    "        predictions = model(x_test_tensor)  # predict  \n",
    "    # calculate RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(predictions,y_test_tensor))\n",
    "    print(f'RBM Test RMSE: {rmse.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Light Gradient-Boosting Machine (LightGBM) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LightGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78890"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(len(x_train)*0.9)#[:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init LightGBM dataset\n",
    "lgb_sep1 = int(len(x_train)*0.9)\n",
    "#lgb_sep2 = int(len(x_train)*0.9)\n",
    "x_train_lgb = x_train[:lgb_sep1]\n",
    "y_train_lgb = y_train[:lgb_sep1]\n",
    "x_valid_lgb = x_train[lgb_sep1:]\n",
    "y_valid_lgb = y_train[lgb_sep1:]\n",
    "x_test_lgb = x_test\n",
    "y_test_lgb = y_test\n",
    "\n",
    "train_data_lgb = lgb.Dataset(x_train_lgb, label=y_train_lgb)\n",
    "valid_data_lgb = lgb.Dataset(x_valid_lgb, label=y_valid_lgb)\n",
    "test_data_lgb = lgb.Dataset(x_test_lgb, label=y_test_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#find the best num_round parameter\n",
    "if not os.path.isfile('lgb_model.mdl'):\n",
    "    num_round_list=[100,1000,5000,10000,150000]\n",
    "    rmse_list=[]\n",
    "    for lra in num_round_list:\n",
    "        print(f'parameter : {lra}')\n",
    "        # set LightGBM parameters\n",
    "        params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'num_leaves': 31,\n",
    "            'learning_rate': 0.05,\n",
    "            'feature_fraction': 0.9,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 5,\n",
    "            'verbose': 1\n",
    "        }\n",
    "        num_round = lra\n",
    "        bst_lgb = lgb.train(params, train_data_lgb, num_round, valid_sets=[valid_data_lgb])\n",
    "        y_pred_lgb = bst_lgb.predict(x_test_lgb)\n",
    "        # Calculate RMSE\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_lgb, y_pred_lgb))\n",
    "        #print(f'Test RMSE: {rmse:.4f}')\n",
    "        rmse_list.append(rmse)\n",
    "        print(f'lgb Test RMSE: {rmse.item():.4f}')\n",
    "    line_plot(num_round_list,rmse_list,'LightGBM Test RMSE vs num_round','num_round','RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rmse_list[0.8502911544507068,0.8111332466531495,0.8230329609698332,0.8342094066973266,0.863834892190296]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the best learning_rate parameter\n",
    "if not os.path.isfile('lgb_model.mdl'):\n",
    "    learning_rate_list=[0.1,0.01,0.001,0.0001]\n",
    "    rmse_list=[]\n",
    "    for lra in learning_rate_list:\n",
    "        print(f'parameter : {lra}')\n",
    "        # set LightGBM parameters\n",
    "        params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'num_leaves': 31,\n",
    "            'learning_rate': lra,\n",
    "            'feature_fraction': 0.9,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 5,\n",
    "            'verbose': 1\n",
    "        }\n",
    "        num_round = 1000\n",
    "        bst_lgb = lgb.train(params, train_data_lgb, num_round, valid_sets=[valid_data_lgb])\n",
    "        y_pred_lgb = bst_lgb.predict(x_test_lgb)\n",
    "        # Calculate RMSE\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_lgb, y_pred_lgb))\n",
    "        #print(f'Test RMSE: {rmse:.4f}')\n",
    "        rmse_list.append(rmse)\n",
    "        print(f'lgb Test RMSE: {rmse.item():.4f}')\n",
    "    line_plot(learning_rate_list,rmse_list,'LightGBM Test RMSE vs learning_rate','learning_rate','RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rmse_list[0.8136394908712906,0.8310472198009059,0.9211281764371869,1.0629043804696792]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('lgb_model.mdl'):\n",
    "    # set LightGBM parameters\n",
    "    params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.1,\n",
    "        'feature_fraction': 0.9,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': 1\n",
    "    }\n",
    "    # train lightgbm model\n",
    "    num_round = 1000\n",
    "    bst = lgb.train(params, train_data_lgb, num_round, valid_sets=[valid_data_lgb])\n",
    "    bst.save_model('lgb_model.mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM Test RMSE: 0.8136\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile('lgb_model.mdl'):\n",
    "    lgb_model = lgb.Booster(model_file='lgb_model.mdl')\n",
    "    # predict test data\n",
    "    y_pred_lgb = lgb_model.predict(x_test_lgb)\n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_lgb, y_pred_lgb))\n",
    "    #rmse = root_mean_squared_error(y_test_lgb, y_pred_lgb)\n",
    "    #print(f\"Validation set RMSE is: {rmse}\")\n",
    "    #print(f'Test RMSE: {rmse:.4f}')\n",
    "    print(f'LightGBM Test RMSE: {rmse.item():.4f}')\n",
    "else:\n",
    "    # predict valid data\n",
    "    y_pred_valid_lgb = bst.predict(x_valid_lgb)\n",
    "    #y_pred = bst.predict(test_data_lgb)\n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_valid_lgb, y_pred_valid_lgb))\n",
    "    #rmse = root_mean_squared_error(y_valid_lgb, y_pred_valid_lgb)\n",
    "    #print(f\"Validation set RMSE is: {rmse}\")\n",
    "    print(f'valid RMSE: {rmse.item():.4f}')\n",
    "    # predict test data\n",
    "    y_pred_lgb = bst.predict(x_test_lgb)\n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_lgb, y_pred_lgb))\n",
    "    #rmse = root_mean_squared_error(y_test_lgb, y_pred_lgb)\n",
    "    #print(f\"Validation set RMSE is: {rmse}\")\n",
    "    #print(f'Test RMSE: {rmse:.4f}')\n",
    "    print(f'Test RMSE: {rmse.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1636,
     "sourceId": 792972,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
